\chapter{Introduction}
\label{chp:Intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Introduction} 
Most sciences today use mathematical and computer simulation models to approximate the real-world processes under study \cite{Kennedy,Fojo,Vanni}. For example, models play a significant role in health policymaking by estimating the impact of interventions in situations where empirical studies may be time-consuming, costly and impractical \cite{Stout}. Developing a model calls for a trade-off between computational cost and accuracy; simple models require little computation time but can be a poor description of the real-world process, whereas complex models allow for a more accurate description of the process at the cost of increased computational cost. After model development, it is imperative to know how well the model represents reality. Model calibration, or fitting the model to data, increases the confidence that the model provides a realistic approximation to the real-world process \cite{Vanni,Stout} but this process involves running the model many times. This becomes a problem with complex models for which model run-times are very long (Hladish).     
    
 Calibration is the process of comparing model outputs with empirical data to identify the model parameter values that achieve a good fit to data \cite{Menzies,Vanni}. Researchers commonly use calibration methods to find parameter values in case parameter estimates are not available in the literature  \cite{Elske}. The main components of calibration are; summary statistics (targets), the parameter-search strategy, the goodness-of-fit (GOF) measure, acceptance criteria and stopping rules.  Improving the computational cost of the calibration involves specification of a more efficient parameter search strategy. In this study, we focus on sampling algorithms as the parameter search strategy, since sampling methods obtain valid estimates of parameter uncertainty and correlations between parameters. Several more efficient sampling algorithms have been proposed and the number of studies that apply these algorithms is proliferating  \cite{Vanni}. 
 
Existing literature compares the performance of alternative algorithms for calibrating the same model but does not allow us to draw general conclusions \cite{Dahabreh, Minter}. \cite{Hazelbag} highlights the need for simulation-based studies that inform the choice of the parameter search strategy in terms of correct estimation of the posterior in different scenarios in terms of contextual variables (i.e. the number of target statistics, the number of calibrated parameters).  the performance, strengths and limitations of different model calibration methods. algorithm implementation vs “How the algorithm works on paper”


 
 
Because there are many model calibration methods with little or no consensus on their performance, we perform a simulation study to compare the performance of model calibration methods using a simple stochastic Susceptible-Infected-Recovered (SIR) model. The methods to be compared are Rejection Approximate Bayesian Computation (Rejection ABC), Sequential Approximate Bayesian Computation (Sequential ABC) and Bayesian Maximum Likelihood estimation (BMLE).\\

Outline to be completed when thesis is fully written…….

